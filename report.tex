\documentclass[10pt]{article}
\title{\textbf{Machine Learning Project Report }}
\author{Michele Orr\`u}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{mathtools}
\begin{document}
\maketitle

\begin{abstract}
The purpose of this project is to implement a two-step analysis and classification of magnetoencephalography (MEG) data. 
\end{abstract}



\section{Introduction}
Given a user shifting his attention to the left or to the right and monitoring the spatial and temporal status of the MEG\cite{Biomag2010} signal, the goal is to build a classifier capable of brain-decode \textit{a-posteriori} towards which direction the user was actually shifting his attention.



\section{Dataset}
The phenomenom can be described as a set $\{(trial_i, direction_i)\}_{i= 1, 2, \dots, trials}$, where:
$$
trial  = 
 \begin{bmatrix}
   freq_{0, 0} & freq_{0, 1} & \cdots & freq_{0, t}  \\
   freq_{1, 0} & freq_{1, 1} & \cdots & freq_{1, t}  \\
   \vdots      & \vdots      & \ddots & \vdots       \\
   frew_{n, 0} & freq_{n, 1} & \cdots & freq_{n, t} 
 \end{bmatrix}
\hspace{30pt}
 direction = \left\{
 \begin{array}{l l}
   -1 & \text{if left}\\
   +1 & \text{if right}\\
 \end{array}\right .
$$
$freq$ is the MEG frequency, sonded for $t$ bands on each of the $n$ channels.

More precisely, $freq$ encodes the mean $t=6$ different bands:
$2-4Hz, 4-8Hz, 8-13Hz, 13-20Hz, 20-35Hz, 35-46Hz$ taken from a single ($1$) subject on $n=274$ channels. 
Globally, we have $255$ trials ($127$ left, $128$ right).  

Input data is a precious resource, and given that at least $10\%$ is going to be used for testing, 

\subsection{TODO}
\begin{enumerate}
\item \`e veramente la media delle varie frequenze quella che entra in gioco? 
\item la percentuale di dati per fare il test \`e veramente il dieci per cento?
\end{enumerate}



\section{Problem}
The problem of classifying the shift of attention to
the left ($-1$) or to the right ($-1$) has been
approached by stacking/cite{wolpert92} two different
models. The first one consists in a vector of
learning functions, one for each channel, expected to
predict the \emph{local} direction; the second one
instead, is expected to predict the \emph{global}
output, given the local information.

Reasons for this choices are to be found in a reduction of the dimensionality of the feature space, and into a better balance between the number of instances and the number of features.
Furthermore, by stacking, we should presumibly
achieve a most accurate learning model extending the crossvalidation concept, as described in (Wolpert, 1992, section )\cite{wolpert92}, and in a reserach interest concerning localized informations of the brain.


\subsection{Formalization} 
Some introduction here....
\paragraph{First Classifier} we are dealing with a vector of functions
$ \{f_0, f_1, \dots, f_n \}$ 
, one for each channel, defined as:
$$
\begin{array}{lr}
f_i: (\begin{array}{l l l} freq_{i, j}, & \cdots, & freq_{i, j} \end{array}) \to [-1, 1],
&
i \in \{0, \dots, n)
\end{array}
$$ 
 expressing the best direction ($left=-1$, $right=+1$), in terms of confidence within the interval $[-1, +1]$.

\paragraph{Second clasifier} we are looking for a funciton expressing the global output as a boolean value, giventhe localized information about each channel previously discovered and aggregated:
$$
g: [-1, +1]^n \to \{-1, +1\}
$$


\subsection{Learning Model}
Learning model for the first classifier has been chosen to be \textbf{Naive Bayesan}. In fact non ha senso presupporre indipendenza del modello in una rete come il cervello, but for didactic purposes ci hanno insegnato solo questo.
 For the second case instead, I assumed the classifier $g$ to be  a non-probabilistic, binary and linear, hence I opted for \textbf{SVM} related to the second part of the course.

\subsection{Evaluations for Stacking}
describe here the process of evaluation for stacked classifers, sketching the procedure to train and test the proposed stacked architecture.


\section{Implementation}
The implementation is done in pure 
\href{https://python.org/}{python}
, using the
\href{http://scikit-learn.org/stable/}{scikit-learn}
module. Documentatios has been automatically generated by
\href{http://sphinx-doc.org/}{Sphinx}, the most widely used documentation generator in pyton.

Source code is available on 
\href{https://github.com/mmaker/braindecoding}{GitHub}.

\subsection{Code Structure}
bulabulabula
\subsection{Crucial Parts}
write here some algorithmic using \href{https://en.wikibooks.org/wiki/LaTeX/Algorithms_and_Pseudocode}{pseudocodice per pseudoscienziati}


\section{Conclusions}
Machine Learning rocks.

\begin{thebibliography}{9}

\bibitem{Biomag2010}
Marcel van Gerven, Ole Jensen,
\emph{Attention modulations of posterior alpha as a control signal for two-dimensional brainâ€“computer interfaces},
Journal of Neuroscience Methods, 2009

\bibitem{wolpert92}
David H. Wolpert,
\emph{Stacked Generalization},
Neural Networks, 1992

\end{thebibliography}

\end{document}


