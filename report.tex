\documentclass[10pt]{article}
\title{\textbf{Machine Learning Project Report }}
\author{Michele Orr\`u}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{unitsdef}

\begin{document}
\maketitle

\begin{abstract}
The purpose of this project is to implement a two-step analysis and classification of magnetoencephalography (MEG) data. 
\end{abstract}



\section{Introduction}
Given a user shifting his attention to the left or to the right and monitoring the spatial and temporal status of the MEG\cite{Biomag2010} signal, the goal is to build a classifier capable of brain-decode \textit{a-posteriori} towards which direction the user was actually shifting his attention.



\section{Dataset}
The phenomenom can be described as a set $\{(trial_i, direction_i)\}_{i= 1, 2, \dots, trials}$, where:
$$
trial_i  = 
 \begin{bmatrix}
   freq_{0, 0} & freq_{0, 1} & \cdots & freq_{0, t}  \\
   freq_{1, 0} & freq_{1, 1} & \cdots & freq_{1, t}  \\
   \vdots      & \vdots      & \ddots & \vdots       \\
   freq_{n, 0} & freq_{n, 1} & \cdots & freq_{n, t} 
 \end{bmatrix}
\hspace{30pt}
 direction_i = \left\{
 \begin{array}{l l}
   -1 & \text{if left}\\
   +1 & \text{if right}\\
 \end{array}\right .
$$

More precisely, $trial_i \in \mathbb{R} ^{n+t}$ represents over each row one of the $n=274$ channels sonded, and over each column the mean for each of the $t=6$ different band ranges, respectively:
$\{2..4, 4..8, 8..13, 13..20, 20..35, 35..46\}$ $\hertz$ .

Globally, we have $255$ trials ($127$ left, $128$ right) taken over one single ($1$) subject.  


\section{Problem}
The problem of classifying the shift of attention to
the left ($-1$) or to the right ($-1$) has been
approached by stacking\cite{Wolpert92} two different
models. The first one consists in a vector of
learning functions, one for each channel, expected to
predict the \emph{local} direction; the second one
instead, is expected to predict the \emph{global}
output, given the local information.

Reasons for this choices are to be found in a reduction of the dimensionality of the feature space, and into a better balance between the number of instances and the number of features.
Furthermore, by stacking, we should presumibly
achieve a most accurate learning model extending the crossvalidation concept, as described in (Wolpert, 1992, Introduction)\cite{Wolpert92}, and in a reserach interest concerning localized informations of the brain.


\subsection{Formalization} 

\paragraph{First Step} is a set of function 
$ \{f_0, f_1, \dots, f_n \}$ 
taking as input the frequency bands, for a specific trial over all trials and expressing the best direction ($left=-1$, $right=+1$), in terms of confidence within the interval $[-1, +1]$. Hence, we can define them as:
$$
\begin{array}{lr}
f_k: (\begin{array}{l l l} freq_{k, 0}, & \cdots, & freq_{k, t} \end{array})^{trials} \to [-1, 1]^{trials},
&
k \in \{0, \dots, n\}
\end{array}
$$ 


\paragraph{Second Step} we are looking for a funciton expressing the global output as a boolean value, given the localized information about each channel previously discovered and then aggregated:
$$
g: [-1, +1]^n \to \{-1, +1\}
$$


\subsection{Learning Model}
Learning model for the first classifier has been chosen to be \textbf{Naive Bayesan}. In fact non ha senso presupporre indipendenza del modello in una rete come il cervello, but for didactic purposes ci hanno insegnato solo questo.
 For the second case instead, I assumed the classifier $g$ to be  a non-probabilistic, binary and linear, hence I opted for \textbf{SVM} related to the second part of the course.


\subsection{Evaluations for Stacking}
describe here the process of evaluation for stacked classifers, sketching the procedure to train and test the proposed stacked architecture.


\subsection{Training}
ho to train data?


\subsection{Validation}
use 10-fold crossvalidation.



\section{Implementation}
The implementation is done in pure 
\href{https://python.org/}{python}
, using the
\href{http://scikit-learn.org/stable/}{scikit-learn}
module. Documentatios has been automatically generated by
\href{http://sphinx-doc.org/}{Sphinx}, the most widely used documentation generator in pyton.

Source code is available on 
\href{https://github.com/mmaker/braindecoding}{GitHub}.

\subsection{Code Structure}
bulabulabula
\subsection{Crucial Parts}
write here some algorithmic using \href{https://en.wikibooks.org/wiki/LaTeX/Algorithms_and_Pseudocode}{pseudocodice per pseudoscienziati}


\section{Conclusions}
Machine Learning rocks.

\begin{thebibliography}{9}

\bibitem{Biomag2010}
Marcel van Gerven, Ole Jensen,
\emph{Attention modulations of posterior alpha as a control signal for two-dimensional brainâ€“computer interfaces},
Journal of Neuroscience Methods, 2009


\bibitem{Wolpert92}
David H. Wolpert,
\emph{Stacked Generalization},
Neural Networks, 1992

\bibitem{Braindecoding}
E. Olivetti, A. Mognon, S. Greiner and P. Avesani, 
\emph{Brain Decoding: Biases in Error Estimation}, 
Brain Decoding Workshop, 2010

\end{thebibliography}

\end{document}


